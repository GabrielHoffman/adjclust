---
title: "Notes on CHAC implementation in adjclust"
author: "Pierre Neuvial, Nathalie Villa-Vialaneix"
date: "`r Sys.Date()`"
output: 
  BiocStyle::html_document
vignette: >
  %\VignetteIndexEntry{Notes on CHAC implementation in adjclust}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

This document aims at clarifying relations between dissimilarity and similarity 
methods for hierarchical clustering and at explaining the choices made for the
implementation in ```adjclust```. In the remaining of this document, we 
suppose given $n$ objects, $\{1, \ldots, n\}$ that have to be clustered in a
contiguous way. 

# Basic implementation of CHAC in ```adjclust```

The base implementation of ```adjclust``` takes, as an input, a kernel $k$ which
is supposed to be symmetric and positive (in the kernel sense) and to satisfy:
\[
	k(i,i) = 1,\qquad \forall\, i=1,\ldots,n.
\]

If your data are under this format, then the constrained clustering can be 
performed with 
```{r ex-sim, eval=FALSE}
fit <- adjClust(k, type = "similarity")
```
or with
```{r ex-sim2, eval=FALSE}
fit <- adjClust(k, type = "similarity", h = h)
```
if, in addition, the kernel $k$ is supposed to take only null entries outside
of a diagonal of size ```h```.

The implementation is the one described in [1].


# More advanced used for kernel or similarity matrices

## Non positive but normalized similarities

In this section, the available data set is a matrix $s$ that can either have 
only positive entries (in this case it is called a similarity) or both positive 
and non positive entries. If, in addition, $s(i,i) = 1$ for all $i=1,\ldots,n$ 
and $s(i,j) \leq 1$ for all $i,j=1,\ldots,n$ then the algorithm implemented in
```adjclust``` can be applied directly, similarly as for a standard kernel
(previous section on "Basic implementation"). 

The interpretation is similar to the kernel case, under the assumption that 
small similarity values or similarity values that are strongly negative are less 
expected to be clustered together than large similarity values. The application
of the method is justified by the fact that, for a given matrix $s$ described
as above, we can find a $\lambda > 0$ such that the matrix $k_\lambda$ defined
by
\[
  \forall\,1,\ldots,n,\qquad k_\lambda(i,j) = s(i,j) + \lambda 
  \mathbb{I}_{\{i=j\}}
\]
is a kernel (*i.e.*, the matrix $k_\lambda = s + \lambda \mathbb{I}$ is positive 
definite; hence, $\lambda$ has only to be any real number larger than the 
opposite of the smallest negative eigenvalue of $s$). [2] show that the HAC 
obtained from the distance induced by the kernel $k_\lambda$ in its feature 
space and the HAC obtained from the *ad-hoc* dissimilarity
\[
  \forall\, i,j=1,\ldots,n,\qquad d(i,j) = \sqrt{s(i,i) + s(j,j) - 2s(i,j)} = 
  \sqrt{2(1-s(i,j))}
\]
are the same except that the merging levels are all shifted by $\lambda$. 

In conclusion, to address this case, the command lines that have to be used are
the ones described in the previous section.


## Non normalized similarities

Suppose now that the data set is described by a matrix $s$ as in the previous
section except that its diagonal has arbitrary entries and eventually, there is
at least one couple $i,j\in\{1,\ldots,n\}$ such that 
\[
  2s(i,j) > s(i,i) + s(j,j).
\]

These properties posed problem in the implementation of constrained HAC
described in [1] because:

* the original implementation silently assumed that diagonal entries are all 
equal to 1;

* the original implementation silently relied on the implicit dissimilarity
\[
  \forall\,i,j=1,\ldots,n,\qquad d^2(i,j) = s(i,i) + s(j,j) - 2s(i,j)
\]
and this has no meaning in the case where $2s(i,j) > s(i,i) + s(j,j)$ for at 
least one couple $i,j\in\{1,\ldots,n\}$.

In this case, the package performs the following transformations:

* a matrix $\tilde{s}$ is defined as
\[
  \forall\,i,j=1,\ldots,n,\qquad \tilde{s}(i,j) = s(i,j) + \lambda 
  \mathbb{I}_{\{i=j\}}
\]
for a $\lambda$ large enough to ensure that $\tilde{s}(i,i) + \tilde{s}(j,j) -
2\tilde{s}(i,j) < 0$ for all $i,j\in \{1,\ldots,n\}$. In the package, $\lambda$
is chosen as
\[
  \lambda := \max_{i,j} \max\{0, s(i,j) - s(i,i)\} + \epsilon
\]
for a small $\epsilon > 0$. This case is justified by the description of the 
property described in the section on non positive but normalized similarities.
The underlying idea is that, shifting the diagonal entries of a similarity 
matrix would not change HAC result and thus they can be shifted until they 
induce a proper *ad-hoc* dissimilarity matrix;

* once a matrix $\tilde{s}$ has been obtained that is such that 
$\tilde{s}(i,i) + \tilde{s}(j,j) - 2\tilde{s}(i,j) < 0$ for all 
$i,j\in \{1,\ldots,n\}$, it is normalized to unit diagonal to allow its use in 
the original implementation as in [1]:
\[
  \forall\,i,j=1,\ldots,n,\qquad \tilde{\tilde{s}} = \tilde{s}(i,j) + 1 - 
  \frac{1}{2} (\tilde{s}(i,i) + \tilde{s}(j,j)).
\]
By definition of $\tilde{\tilde{s}}$, we have that: 1) $\tilde{\tilde{s}}(i,i) =
1$ for all $i=1,\ldots,n$, that 2) $\tilde{\tilde{s}}(i,i) +
\tilde{\tilde{s}}(j,j) - 2 \tilde{\tilde{s}}(i,j) > 0$ for all $i,j=1,\ldots,n$ 
and that 3) the *ad-hoc* dissimilarities induced by $\tilde{s}$ and 
$\tilde{\tilde{s}}$ are identical:
\[
  \forall\,i=1,\ldots,j,\qquad \tilde{\tilde{d}}(i,j) = \tilde{d}(i,j).
\]

**Important remark**: The previous transformation described above affect all the
elements of the similarity matrix and not only its diagonal, contrarily to what
happens to the previous case. Hence, it is not compatible (up to now) with the
fast implementation that requires to have a $h$-band centered on the diagonal 
outside of which all entries are supposed to be 0. The current behavior of the
package consists in forbiding the use of the fast implementation as long as the
provided matrix is not normalized (all diagonal entries equal to 1). For all the
other case, the constrained clustering from $s$, including the proper matrix
transformations, is available with:
```{r ex-notnormsim, eval=FALSE}
fit <- adjClust(s, type = "similarity")
```


## Case of dissimilarity data

The original implementation of HAC requires that a dissimilarity matrix is 
given to the algorithm. However, the implementation of ```adjclust``` is based
on a kernel approach. The case of dissimilaritiy is thus handled as described in
this section.

Suppose given a dissimilarity $d$ which satisfies:

* $d$ has non negative entries: $d(i,j) \geq 0$ for all $i=1,\ldots,n$;

* $d$ is symmetric: $d(i,j) = d(j,i)$ for all $i,j=1,\ldots,n$;

* $d$ has a null diagonal: $d(i,i) = 0$ for all $i=1,\ldots,n$.

Any sequence of positive numbers $(a_i)_{i=1,\ldots,n}$ would provide a 
similarity $s$ for which $d$ is the *ad-hoc* dissimilarity by setting:
\[
  \left\{ \begin{array}{l}
    s(i,i) = a_i\\
    s(i,j) = \frac{1}{2} (a_i + a_j - d^2(i,j))
  \end{array} \right. .
\]
By definition, such an $s$ satisfies the property $2s(i,j) > s(i,i) + s(j,j)$
and, any choices for $(a_i)_{i=1,\ldots,n}$ would provide the same clustering
(since they all yield to the same *ad-hoc* dissimilarity). To satisfy the 
requirement of the implementation in ```adjclust```, the choice $a_i = 1$ for 
all $i=1,\ldots,n$ has been made.

**Important remark**: The sparse implementation does not cover the case of
dissimilarity matrices. The constrained clustering is thus performed with:
```{r ex-dissim, eval=FALSE}
fit <- adjClust(d, type = "dissimilarity")
```


# References

[1] Dehman A. (2015). [Spatial clustering of linkage disequilibrium blocks for genome-wide association studies](https://tel.archives-ouvertes.fr/tel-01288568/). Phd Thesis, 
UniversitÃ© Paris Saclay.

[2] Miyamoto S., Abe R., Endo Y., Takeshita J. (2015) Ward method of 
hierarchical clustering for non-Eclidean similarity measures. In: *Proceedings
of the VIIth International Conference of Soft Computing and Pattern 
Recognition* (SoCPaR 2015).
